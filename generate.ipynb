{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the saved data\n",
    "X = np.load('notes.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 1.7865,  1.8281, 72.0000, 76.0000],\n",
      "        [ 1.8031,  2.0000, 67.0000, 56.0000],\n",
      "        [ 1.9833,  2.0979, 74.0000, 68.0000],\n",
      "        [ 2.0375,  2.1063, 72.0000, 77.0000],\n",
      "        [ 2.0979,  2.1823, 74.0000, 51.0000],\n",
      "        [ 2.1719,  2.2802, 67.0000, 57.0000],\n",
      "        [ 2.3281,  2.5094, 66.0000, 58.0000],\n",
      "        [ 2.1490,  2.5198, 72.0000, 60.0000],\n",
      "        [ 1.9833,  2.5229, 57.0000, 61.0000],\n",
      "        [ 2.5229,  2.5896, 71.0000, 68.0000],\n",
      "        [ 2.5906,  2.6740, 72.0000, 47.0000],\n",
      "        [ 2.5583,  2.7635, 64.0000, 35.0000],\n",
      "        [ 2.8875,  3.1135, 62.0000, 63.0000],\n",
      "        [ 3.0802,  3.2729, 66.0000, 63.0000],\n",
      "        [ 2.7125,  3.4198, 59.0000, 50.0000],\n",
      "        [ 2.6792,  3.4479, 74.0000, 68.0000]]), tensor([ 3.4531,  3.6021, 71.0000, 66.0000]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MusicSequenceDataset(Dataset):\n",
    "    def __init__(self, notes, sequence_length):\n",
    "        self.notes = notes  # list of lists of dictionaries (list of sequences of notes)\n",
    "        self.sequence_length = sequence_length  # length of the input sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.notes) - self.sequence_length  # Total number of sequences available\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get a sequence of notes (list of dictionaries)\n",
    "        sequence = self.notes[idx:idx + self.sequence_length]\n",
    "        target = self.notes[idx + self.sequence_length]\n",
    "\n",
    "        # The sequence should now be a flat list of dictionaries, create the tensor for X\n",
    "        X = torch.tensor([[note['start'], note['end'], note['pitch'], note['velocity']] for note in sequence], dtype=torch.float32)\n",
    "        \n",
    "        # The target note is the next note's features\n",
    "        y = torch.tensor([target['start'], target['end'], target['pitch'], target['velocity']], dtype=torch.float32)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "class MusicLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_size=128, num_layers=2, output_size=4):\n",
    "        super(MusicLSTMModel, self).__init__()\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to map the hidden state to the output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass through LSTM layer\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        # Only take the output of the last time step\n",
    "        last_lstm_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Pass the last output through a fully connected layer\n",
    "        output = self.fc(last_lstm_output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# Instantiate the dataset and DataLoader\n",
    "sequence_length = 16\n",
    "dataset = MusicSequenceDataset(X, sequence_length=sequence_length)\n",
    "print(dataset[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MusicLSTMModel(\n",
       "  (lstm): LSTM(4, 128, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = MusicLSTMModel()\n",
    "\n",
    "# Load the weights from the .pth file\n",
    "model.load_state_dict(torch.load('model_epoch_3.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[107.0417, 107.2552,  67.0000,  62.0000],\n",
      "         [107.2500, 107.3917,  50.0000,  61.0000],\n",
      "         [107.2458, 107.4875,  71.0000,  76.0000],\n",
      "         [107.4437, 107.6698,  67.0000,  59.0000],\n",
      "         [107.6448, 107.8031,  72.0000,  76.0000],\n",
      "         [107.6521, 107.8552,  51.0000,  66.0000],\n",
      "         [107.8333, 108.1635,  67.0000,  66.0000],\n",
      "         [108.0708, 108.2135,  75.0000,  78.0000]]])\n",
      "tensor([[-57.9143, -57.3515,  44.5148,  41.7698]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 4 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Generate music with the model\u001b[39;00m\n\u001b[0;32m     57\u001b[0m num_notes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m  \u001b[38;5;66;03m# Desired number of notes to generate\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m generated_notes \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_music\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_notes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_notes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Convert the generated notes to a MIDI file\u001b[39;00m\n\u001b[0;32m     61\u001b[0m notes_to_midi(generated_notes, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_music.mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 22\u001b[0m, in \u001b[0;36mgenerate_music\u001b[1;34m(model, initial_notes, num_notes, sequence_length)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted_note)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Extract the predicted note (start, end, pitch, velocity)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m predicted_note_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mpredicted_note\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m: predicted_note[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpitch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(predicted_note[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()),\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvelocity\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(predicted_note[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     26\u001b[0m }\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Append the predicted note to the sequence\u001b[39;00m\n\u001b[0;32m     29\u001b[0m notes_sequence\u001b[38;5;241m.\u001b[39mappend(predicted_note_dict)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: a Tensor with 4 elements cannot be converted to Scalar"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pretty_midi\n",
    "import random\n",
    "\n",
    "# Function to generate music\n",
    "def generate_music(model, initial_notes, num_notes, sequence_length=8):\n",
    "    notes_sequence = initial_notes[0]\n",
    "    \n",
    "    # Generate new notes until we have the required number of notes\n",
    "    for i in range(num_notes - len(initial_notes)):\n",
    "        # Create input tensor from the last `sequence_length` notes\n",
    "        sequence = notes_sequence[i:i+sequence_length].unsqueeze(0) # add the batch dimension\n",
    "        print(sequence)\n",
    "        \n",
    "        # Predict the next note\n",
    "        with torch.no_grad():\n",
    "            predicted_note = model(sequence)\n",
    "\n",
    "        print(predicted_note)\n",
    "\n",
    "        # Extract the predicted note (start, end, pitch, velocity)\n",
    "        predicted_note_dict = {\n",
    "            'start': predicted_note[0].item(),\n",
    "            'end': predicted_note[1].item(),\n",
    "            'pitch': int(predicted_note[2].item()),\n",
    "            'velocity': int(predicted_note[3].item())\n",
    "        }\n",
    "        \n",
    "        # Append the predicted note to the sequence\n",
    "        notes_sequence.append(predicted_note_dict)\n",
    "    \n",
    "    return notes_sequence\n",
    "\n",
    "# Function to convert the list of notes to a MIDI file\n",
    "def notes_to_midi(notes, output_file='generated_music.mid'):\n",
    "    midi = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.Instrument(program=pretty_midi.program_to_instrument(0))  # 0 is the piano program\n",
    "    \n",
    "    for note in notes:\n",
    "        midi_note = pretty_midi.Note(\n",
    "            velocity=note['velocity'],\n",
    "            pitch=note['pitch'],\n",
    "            start=note['start'],\n",
    "            end=note['end']\n",
    "        )\n",
    "        instrument.notes.append(midi_note)\n",
    "    \n",
    "    midi.instruments.append(instrument)\n",
    "    midi.write(output_file)\n",
    "    print(f'MIDI file saved to {output_file}')\n",
    "\n",
    "# Initial notes to start the sequence (replace with your own starting notes)'\n",
    "#rnd_idx = random.randint(0, len(dataset)-1)\n",
    "\n",
    "initial_notes = dataset[872]\n",
    "\n",
    "# Generate music with the model\n",
    "num_notes = 200  # Desired number of notes to generate\n",
    "generated_notes = generate_music(model, initial_notes, num_notes)\n",
    "\n",
    "# Convert the generated notes to a MIDI file\n",
    "notes_to_midi(generated_notes, 'generated_music.mid')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
